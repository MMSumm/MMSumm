{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "720c2611-fb63-4803-8ef2-6f7b11f2cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4176c37e-7b39-4baa-b718-35ca27fb4a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89171cdf-a8c2-4a58-bd41-e32cd43c9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fff94ed3-6b86-4ec9-8fe5-a7b35a512f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pytorch_transformers import BertModel, BertTokenizer, BertConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import torch.utils.data as DU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import base64\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "# import gensim.downloader as api\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "from utils.tsv_file import TSVFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb206ab9-66d5-4745-ad5f-8ecdb01189cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oscar.modeling.modeling_bert import ImageBertForSequenceClassification\n",
    "\n",
    "oscar_config = BertConfig.from_pretrained('./models/oscar_ir/')\n",
    "\n",
    "oscar_ir = ImageBertForSequenceClassification.from_pretrained('./models/oscar_ir', config=oscar_config)\n",
    "oscar_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cccf9fbe-eaf3-413e-8350-f11fe381cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a65bf4f-02d5-497d-9fbd-fbbb08f43210",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'\n",
    "IMG_FEAT_DIR = './data/mscoco_imgfeat/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1987797-86f2-48f6-a68e-5dac7690d08b",
   "metadata": {},
   "source": [
    "# DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1da5ea-6694-4d32-aede-a0d5b076d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import cv2\n",
    "from ast import literal_eval\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "import random\n",
    "import spacy\n",
    "from \n",
    "\n",
    "class MSMO_Data(Dataset):\n",
    "    def __init__(self, csv_file, root_dir='/scratch/anshul/valid_data/', transform=None):\n",
    "        self.transform = transform\n",
    "        self.listObj = ['bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'hat', 'backpack', 'shoe', 'sports', 'bottle', 'pizza', 'donut', 'chair', 'tv', 'laptop', 'phone', 'blender', 'book', 'clock', 'refrigerator', 'oven', 'microwave', 'oven', 'keyboard']\n",
    "        self.nlp = spacy.load(\"en_core_web_trf\")\n",
    "        self.wv = api.load('word2vec-google-news-300')\n",
    "        self.df = pd.read_csv(root_dir+csv_file)\n",
    "        self.root_dir = root_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        article_file = self.root_dir+'article/'+self.df['article'][idx]\n",
    "        art = open(article_file)\n",
    "        art = art.read()\n",
    "        art = art.replace(\"\\n\", \" \")\n",
    "        art = art.rstrip()\n",
    "        text = art.split(\"@title \")[1].split(\"@summary \")[0]\n",
    "        doc = self.nlp(text)\n",
    "        ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]\n",
    "        text1 = text\n",
    "        for item in ents:\n",
    "            if(item[1]=='PERSON'):\n",
    "                try:\n",
    "                    vec = wv[item[0]]\n",
    "                except KeyError:\n",
    "                    text1=text1.replace(item[0], 'person')\n",
    "                    continue\n",
    "                if self.wv.similarity(item[0].split(' ')[0], 'man')>self.wv.similarity(item[0].split(' ')[0], 'woman'):\n",
    "                    text1=text1.replace(item[0], 'man')\n",
    "                else:\n",
    "                    text1=text1.replace(item[0], 'woman')\n",
    "                \n",
    "            else:\n",
    "                maxSim=0.1\n",
    "                rep = item[0]\n",
    "                try:\n",
    "                    vec = wv[item[0]]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                for obj in self.listObj:\n",
    "                    if self.wv.similarity(item[0].split(' ')[0], obj) > maxSim:\n",
    "                        maxSum=self.wv.similarity(item[0].split(' ')[0], obj)\n",
    "                        rep=obj\n",
    "                text1=text1.replace(item[0], rep)\n",
    "        \n",
    "        #tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        #text_tokens = tokenizer(text, padding='max_length', max_length=3000)\n",
    "\n",
    "        \n",
    "        summaries = art.split(\"@title \")[1].split(\"@summary \")[1:]\n",
    "        summary = \"\"\n",
    "        for i in summaries:\n",
    "            i = i.rstrip()\n",
    "            summary+=i\n",
    "            summary+=\". \"\n",
    "        \n",
    "        #summary_tokens = tokenizer(summary, padding='max_length', max_length=300)   \n",
    "\n",
    "        imgs = []\n",
    "        x = literal_eval(self.df['img'][idx])\n",
    "        k=len(x)\n",
    "        arr = [i for i in range(k)]\n",
    "        if len(x)>15:\n",
    "            k=15\n",
    "            arr = random.sample(range(len(x)), 15)\n",
    "        for i in arr:\n",
    "            img_name = self.root_dir+'img/'+x[i]\n",
    "            im = cv2.imread(img_name)\n",
    "            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "            im = cv2.resize(im, (150, 150))\n",
    "            if(self.transform):\n",
    "                im = self.transform(im)\n",
    "            imgs.append(im)\n",
    "        for i in range(15-k):\n",
    "            imgs.append(np.zeros((150, 150, 3)))\n",
    "        \n",
    "        data_pt = {'text':text, 'obj_text':text1, 'summary':summary, 'im_list':imgs}\n",
    "        return data_pt\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf9aaf-e0f2-4c86-a06d-ca7f8e3776aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MSMO_Data('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf194fa5-1816-4278-8420-eef48addf73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d544f-37dd-4f25-8351-637f49b5076c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a5999-4efc-4a1e-8ff1-404d4fa9c691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a776f636-1162-4c3e-ab09-fa2745969eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OscarInput:\n",
    "    def __init__(self):\n",
    "        IMG2IDX_JSON = './img2idx.json'\n",
    "        \n",
    "        with open(IMG2IDX_JSON, 'r') as f:\n",
    "            self.img2idx = json.load(f)\n",
    "            \n",
    "        self.features_file = TSVFile(os.path.join(IMG_FEAT_DIR, 'features.tsv'))\n",
    "        self.labels_file = TSVFile(os.path.join(IMG_FEAT_DIR, 'labels.tsv'))\n",
    "        \n",
    "        self.tokenizer = oscar_tokenizer\n",
    "        self.max_seq_len = 70\n",
    "        self.max_img_seq_len = 70\n",
    "        self.att_mask_type = 'CLR'\n",
    "            \n",
    "    def make_img2idx(self):\n",
    "        IMG_FILE = os.path.join(IMG_FEAT_DIR, 'features.tsv')\n",
    "        LABEL_FILE = os.path.join(IMG_FEAT_DIR, 'labels.tsv')\n",
    "        img2idx = {}\n",
    "        with open(IMG_FILE, 'r') as img_file:\n",
    "            with open(LABEL_FILE, 'r') as label_file:\n",
    "                cnt = 0\n",
    "                line = img_file.readline()\n",
    "                line_label = label_file.readline()\n",
    "                while line:\n",
    "                    img_id = line.split('\\t')[0].strip()\n",
    "                    label_id = line.split('\\t')[0].strip()\n",
    "\n",
    "                    if img_id == label_id:\n",
    "                        img2idx[img_id] = cnt\n",
    "                        cnt += 1\n",
    "                    else:\n",
    "                        print('mismatch... How?')\n",
    "\n",
    "                    line = img_file.readline()\n",
    "\n",
    "        with open('./img2idx.json', 'w+') as f:\n",
    "            json.dump(img2idx, f)\n",
    "\n",
    "    def get_oscar_input(self, sent, img_id):\n",
    "        idx = self.img2idx[img_id]\n",
    "        features_row = self.features_file.seek(idx)\n",
    "        labels_row = self.labels_file.seek(idx)\n",
    "        \n",
    "        # GET LABELS\n",
    "        results = json.loads(labels_row[1].replace('\\'', '\\\"'))\n",
    "        objects = results['objects'] if type(\n",
    "            results) == dict else results\n",
    "        labels = {\n",
    "            \"image_h\": results[\"image_h\"] if type(\n",
    "                results) == dict else 600,\n",
    "            \"image_w\": results[\"image_w\"] if type(\n",
    "                results) == dict else 800,\n",
    "            \"class\": [cur_d['class'] for cur_d in objects],\n",
    "            \"boxes\": np.array([cur_d['rect'] for cur_d in objects],\n",
    "                              dtype=np.float32)\n",
    "        }\n",
    "        \n",
    "        od_labels = ' '.join(labels['class'])\n",
    "     \n",
    "        # GET IMAGE FEATURES\n",
    "        num_boxes = int(features_row[1])\n",
    "        features = np.frombuffer(base64.b64decode(features_row[-1]),\n",
    "                                 dtype=np.float32).reshape((num_boxes, -1))\n",
    "        t_features = torch.from_numpy(features)\n",
    "        \n",
    "        # TENSORIZE EXAMPLE\n",
    "                \n",
    "        cls_token_segment_id = 0\n",
    "        pad_token_segment_id = 0\n",
    "        sequence_a_segment_id = 0\n",
    "        sequence_b_segment_id = 1\n",
    "        \n",
    "        text_a = sent\n",
    "        text_b = od_labels\n",
    "        img_feat = t_features\n",
    "        \n",
    "        tokens_a = oscar_tokenizer.tokenize(text_a)\n",
    "        if len(tokens_a) > self.max_seq_len - 2:\n",
    "            tokens_a = tokens_a[:(self.max_seq_len - 2)]\n",
    "\n",
    "        tokens = [self.tokenizer.cls_token] + tokens_a + [self.tokenizer.sep_token]\n",
    "        segment_ids = [cls_token_segment_id] + [sequence_a_segment_id] * (len(tokens_a) + 1)\n",
    "        seq_a_len = len(tokens)\n",
    "        if text_b:\n",
    "            tokens_b = self.tokenizer.tokenize(text_b)\n",
    "            if len(tokens_b) > self.max_seq_len - len(tokens) - 1:\n",
    "                tokens_b = tokens_b[: (self.max_seq_len - len(tokens) - 1)]\n",
    "            tokens += tokens_b + [self.tokenizer.sep_token]\n",
    "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "\n",
    "        seq_len = len(tokens)\n",
    "        seq_padding_len = self.max_seq_len - seq_len\n",
    "        tokens += [self.tokenizer.pad_token] * seq_padding_len\n",
    "        segment_ids += [pad_token_segment_id] * seq_padding_len\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # image features\n",
    "        img_len = img_feat.shape[0]\n",
    "        if img_len > self.max_img_seq_len:\n",
    "            img_feat = img_feat[0 : self.max_img_seq_len, :]\n",
    "            img_len = img_feat.shape[0]\n",
    "            img_padding_len = 0\n",
    "        else:\n",
    "            img_padding_len = self.max_img_seq_len - img_len\n",
    "            padding_matrix = torch.zeros((img_padding_len, img_feat.shape[1]))\n",
    "            img_feat = torch.cat((img_feat, padding_matrix), 0)\n",
    "\n",
    "        # generate attention_mask\n",
    "        att_mask_type = self.att_mask_type\n",
    "        if att_mask_type == \"CLR\":\n",
    "            attention_mask = [1] * seq_len + [0] * seq_padding_len + \\\n",
    "                             [1] * img_len + [0] * img_padding_len\n",
    "            \n",
    "        else:\n",
    "            # use 2D mask to represent the attention\n",
    "            max_len = self.max_seq_len + self.max_img_seq_len\n",
    "            attention_mask = torch.zeros((max_len, max_len), dtype=torch.long)\n",
    "            # full attention of C-C, L-L, R-R\n",
    "            c_start, c_end = 0, seq_a_len\n",
    "            l_start, l_end = seq_a_len, seq_len\n",
    "            r_start, r_end = self.max_seq_len, self.max_seq_len + img_len\n",
    "            attention_mask[c_start : c_end, c_start : c_end] = 1\n",
    "            attention_mask[l_start : l_end, l_start : l_end] = 1\n",
    "            attention_mask[r_start : r_end, r_start : r_end] = 1\n",
    "            if att_mask_type == 'CL':\n",
    "                attention_mask[c_start : c_end, l_start : l_end] = 1\n",
    "                attention_mask[l_start : l_end, c_start : c_end] = 1\n",
    "            elif att_mask_type == 'CR':\n",
    "                attention_mask[c_start : c_end, r_start : r_end] = 1\n",
    "                attention_mask[r_start : r_end, c_start : c_end] = 1\n",
    "            elif att_mask_type == 'LR':\n",
    "                attention_mask[l_start : l_end, r_start : r_end] = 1\n",
    "                attention_mask[r_start : r_end, l_start : l_end] = 1\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported attention mask type {}\".format(att_mask_type))\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long).unsqueeze(0)\n",
    "        segment_ids = torch.tensor(segment_ids, dtype=torch.long).unsqueeze(0)\n",
    "        img_feat = img_feat.unsqueeze(0)\n",
    "        \n",
    "        label = 1\n",
    "        #return idx, tuple([input_ids, attention_mask, segment_ids, img_feat, label])\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': segment_ids,\n",
    "            'img_feats': img_feat,\n",
    "            'labels': None\n",
    "        }\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1a22c492-5679-40c2-9230-489c4197c8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.3426e-06], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oscar_inp = OscarInput()\n",
    "#oscar_inp.make_img2idx()\n",
    "inp = oscar_inp.get_oscar_input(\"\", \"2\")\n",
    "logits = oscar_ir(**inp)[:2][0]\n",
    "sm = nn.Softmax(dim=1)\n",
    "probs = sm(logits)\n",
    "result = probs[:, 1]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c973fe55-1caa-49e8-9a40-239653db16f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11015/965861139.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'im_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset[0]['im_list'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091b525-2f45-4afc-a886-bc48cee06cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "features = np.frombuffer(base64.b64decode('s'*64),\n",
    "                         dtype=np.float32).reshape((4, -1))\n",
    "print(features)\n",
    "t_features = torch.from_numpy(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e97fd-1902-40e0-af26-3384452e5579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b693728-e19a-4943-a718-3e19ca0d00f4",
   "metadata": {},
   "source": [
    "# OSCAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35e2fafe-1d4b-4109-9f12-f7e513a186b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b158ddc-dc5c-4884-a918-758f4bd81bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7984259-50b3-4e43-8402-e60849f2711d",
   "metadata": {},
   "source": [
    "# BERTSUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1087dc-97cc-4549-a834-5b947cc455ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b43be8e7-330a-4158-9172-6901fdc845b3",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c64f0-04bf-4016-962d-58d939907bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMSumm(nn.Module):\n",
    "    def __init__(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d479828-9594-46ce-adfd-4877f77120f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
