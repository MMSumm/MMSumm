[2021-08-22 15:52:49,411 INFO] Loading checkpoint from /scratch/new_msmo/model_step_16000.pt
[2021-08-22 15:52:50,408 INFO] loading configuration file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/config.json
[2021-08-22 15:52:50,417 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-08-22 15:52:50,417 INFO] loading weights file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/pytorch_model.bin
[2021-08-22 15:53:07,178 INFO] Loading test dataset from ../bert_data/test.1.bert.pt, number of examples: 10256
[2021-08-22 15:53:08,343 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2021-08-22 15:53:08,678 INFO] loading lineidx: /scratch/summ_data_imgs/vgg_features.lineidx
[2021-08-22 20:22:17,524 INFO] Calculating Rouge
[2021-08-22 23:03:10,489 INFO] Loading checkpoint from /scratch/new_msmo/model_step_56000.pt
[2021-08-22 23:03:20,534 INFO] loading configuration file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/config.json
[2021-08-22 23:03:20,545 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-08-22 23:03:20,546 INFO] loading weights file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/pytorch_model.bin
[2021-08-22 23:03:37,633 INFO] Loading test dataset from ../bert_data/test.1.bert.pt, number of examples: 10256
[2021-08-22 23:03:38,707 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2021-08-22 23:03:38,971 INFO] loading lineidx: /scratch/summ_data_imgs/vgg_features.lineidx
[2021-08-23 03:10:33,797 INFO] Calculating Rouge
[2021-08-24 11:06:33,095 INFO] Loading checkpoint from /scratch/new_msmo/model_step_16000.pt
[2021-08-24 11:07:52,348 INFO] Loading checkpoint from /scratch/new_msmo/model_step_16000.pt
[2021-08-24 11:08:00,465 INFO] loading configuration file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/config.json
[2021-08-24 11:08:00,466 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-08-24 11:08:00,467 INFO] loading weights file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/pytorch_model.bin
[2021-08-24 11:08:17,611 INFO] Loading test dataset from ../bert_data/test.1.bert.pt, number of examples: 10256
[2021-08-24 11:08:18,750 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2021-08-24 11:08:19,142 INFO] loading lineidx: /scratch/summ_data_imgs/vgg_features.lineidx
[2021-08-24 11:08:56,766 INFO] Loading checkpoint from /scratch/new_msmo/model_step_16000.pt
[2021-08-24 11:09:04,913 INFO] loading configuration file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/config.json
[2021-08-24 11:09:04,914 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-08-24 11:09:04,915 INFO] loading weights file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/pytorch_model.bin
[2021-08-24 11:09:22,643 INFO] Loading test dataset from ../bert_data/test.1.bert.pt, number of examples: 10256
[2021-08-24 11:09:23,681 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2021-08-24 11:09:23,935 INFO] loading lineidx: /scratch/summ_data_imgs/vgg_features.lineidx
[2021-08-24 11:59:40,400 INFO] Loading checkpoint from /scratch/new_msmo/model_step_16000.pt
[2021-08-24 11:59:48,529 INFO] loading configuration file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/config.json
[2021-08-24 11:59:48,539 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-08-24 11:59:48,540 INFO] loading weights file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/pytorch_model.bin
[2021-08-24 12:00:06,869 INFO] Loading test dataset from ../bert_data/test.1.bert.pt, number of examples: 10256
[2021-08-24 12:00:07,984 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2021-08-24 12:00:08,207 INFO] loading lineidx: /scratch/summ_data_imgs/vgg_features.lineidx
[2021-08-24 16:24:46,286 INFO] Calculating Rouge
[2021-08-29 00:07:23,331 INFO] Loading checkpoint from /scratch/new_msmo/negpos_model_76000.pt
[2021-08-29 00:07:34,986 INFO] loading configuration file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/config.json
[2021-08-29 00:07:34,995 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-08-29 00:07:34,996 INFO] loading weights file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/pytorch_model.bin
[2021-08-29 00:07:52,138 INFO] Loading test dataset from ../bert_data/test.1.bert.pt, number of examples: 10256
[2021-08-29 00:07:53,546 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2021-08-29 00:07:53,806 INFO] loading lineidx: /scratch/summ_data_imgs/vgg_features.lineidx
[2021-08-29 04:27:54,541 INFO] Calculating Rouge
[2021-09-06 23:26:05,606 INFO] Loading checkpoint from /scratch/new_msmo/model_step_36000.pt
[2021-09-06 23:26:20,498 INFO] loading configuration file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/config.json
[2021-09-06 23:26:20,507 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-09-06 23:26:20,509 INFO] loading weights file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/pytorch_model.bin
[2021-09-06 23:26:36,058 INFO] Loading test dataset from ../bert_data/test.1.bert.pt, number of examples: 10256
[2021-09-06 23:26:37,205 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2021-09-06 23:26:37,434 INFO] loading lineidx: /scratch/summ_data_imgs/vgg_features.lineidx
[2021-09-07 03:16:59,061 INFO] Calculating Rouge
[2021-10-09 10:39:55,567 INFO] Loading checkpoint from /scratch/new_msmo/model_step_26000.pt
[2021-10-09 10:40:08,433 INFO] loading configuration file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/config.json
[2021-10-09 10:40:08,447 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-10-09 10:40:08,448 INFO] loading weights file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/pytorch_model.bin
[2021-10-09 10:40:24,118 INFO] Loading test dataset from ../bert_data/test.1.bert.pt, number of examples: 10256
[2021-10-09 10:40:25,230 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2021-10-09 10:40:25,488 INFO] loading lineidx: /scratch/summ_data_imgs/vgg_features.lineidx
[2021-10-09 14:51:39,641 INFO] Calculating Rouge
[2021-10-26 03:12:19,239 INFO] Loading checkpoint from /scratch/new_msmo/model_step_26000.pt
[2021-10-26 03:12:38,760 INFO] loading configuration file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/config.json
[2021-10-26 03:12:38,774 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-10-26 03:12:38,774 INFO] loading weights file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/pytorch_model.bin
[2021-10-26 03:12:55,026 INFO] Loading test dataset from ../bert_data/test.1.bert.pt, number of examples: 10256
[2021-10-26 03:12:56,191 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2021-10-26 03:12:56,640 INFO] loading lineidx: /scratch/summ_data_imgs/vgg_features.lineidx
[2021-10-26 03:13:23,524 INFO] Loading checkpoint from /scratch/new_msmo/model_step_26000.pt
[2021-10-26 03:13:42,993 INFO] loading configuration file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/config.json
[2021-10-26 03:13:42,994 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-10-26 03:13:42,995 INFO] loading weights file /home/anshul.padhi/msmo/PreSumm/src/models/presumm-bert/pytorch_model.bin
[2021-10-26 03:13:58,252 INFO] Loading test dataset from ../bert_data/test.1.bert.pt, number of examples: 10256
[2021-10-26 03:13:59,372 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2021-10-26 03:13:59,578 INFO] loading lineidx: /scratch/summ_data_imgs/vgg_features.lineidx
[2021-10-26 07:27:11,632 INFO] Calculating Rouge
[2021-10-26 07:27:22,859 INFO] Writing summaries.
[2021-10-26 07:27:22,867 INFO] Processing summaries. Saving system files to ../temp/tmp1n2vv5fy/system and model files to ../temp/tmp1n2vv5fy/model.
[2021-10-26 07:27:22,867 INFO] Processing files in ../temp/rouge-tmp-2021-10-26-07-27-11/candidate/.
[2021-10-26 07:27:32,688 INFO] Saved processed files to ../temp/tmp1n2vv5fy/system.
[2021-10-26 07:27:32,689 INFO] Processing files in ../temp/rouge-tmp-2021-10-26-07-27-11/reference/.
[2021-10-26 07:27:43,048 INFO] Saved processed files to ../temp/tmp1n2vv5fy/model.
[2021-10-26 07:27:43,285 INFO] Written ROUGE configuration to ../temp/tmpzhvb9rdp/rouge_conf.xml
[2021-10-26 07:27:43,286 INFO] Running ROUGE with command /home/anshul.padhi/msmo_contrastive_loss/PreSumm/src/pyrouge/rouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /home/anshul.padhi/msmo_contrastive_loss/PreSumm/src/pyrouge/rouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpzhvb9rdp/rouge_conf.xml
[2021-10-26 07:29:40,851 INFO] Rouges at step 26000 
>> ROUGE-F(1/2/3/l): 41.95/19.26/39.07
ROUGE-R(1/2/3/l): 44.50/20.41/41.41

